{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4d957d",
   "metadata": {},
   "source": [
    "# TP2 - CLASE 3\n",
    "\n",
    "## 1. Medición de foco en toda la imagen\n",
    "\n",
    "### Objetivo:\n",
    "Implementar un detector de máximo enfoque sobre un video aplicando técnicas de análisis espectral similar al que utilizan las cámaras digitales modernas. El video a procesar será: **focus_video.mov**.\n",
    "\n",
    "Se debe implementar un algoritmo que dada una imagen, o región, calcule la métrica propuesta en el paper **\"Image Sharpness Measure for Blurred Images in Frequency Domain\"** y realizar tres experimentos:\n",
    "\n",
    "1. **Medición sobre todo el frame.**\n",
    "2. **Medición sobre una ROI ubicada en el centro del frame.** Área de la ROI = 5 o 10% del área total del frame.\n",
    "\n",
    "### Opcional:\n",
    "1. **Medición sobre una matriz de enfoque** compuesta por un arreglo de NxM elementos rectangulares equiespaciados. N y M son valores arbitrarios, probar con varios valores 3x3, 7x5, etc. (al menos 3)\n",
    "\n",
    "### Para cada experimento se debe presentar:\n",
    "- Una curva o varias curvas que muestren la evolución de la métrica frame a frame donde se vea claramente cuando el algoritmo detectó el punto de máximo enfoque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f689f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image quality analysis and focus detection libraries.\"\"\"\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Optional, Dict, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd18348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open video file and extract metadata\n",
    "video_path = \"data/focus_video.mov\"\n",
    "\n",
    "if os.path.exists(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = frame_count / fps if fps > 0 else 0\n",
    "    \n",
    "    video_info = {\n",
    "        'frames': frame_count,\n",
    "        'resolution': f\"{width}x{height}\",\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'fps': fps,\n",
    "        'duration_seconds': duration,\n",
    "        'codec': int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    }\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    for key, value in video_info.items():\n",
    "        if key == 'codec':\n",
    "            codec_str = ''.join([chr((value >> 8 * i) & 0xFF) for i in range(4)])\n",
    "            print(f\"{key}: {codec_str}\")\n",
    "        elif key == 'duration_seconds':\n",
    "            print(f\"{key}: {value:.2f}\")\n",
    "        elif key == 'fps':\n",
    "            print(f\"{key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "            \n",
    "else:\n",
    "    print(f\"Video file not found: {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a video showing the Fourier transform of each frame\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "input_path = \"data/focus_video.mov\"\n",
    "output_path = \"outputs/fourier_transform_video.mp4\"\n",
    "\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "for i in range(frame_count):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    f_transform = np.fft.fft2(gray)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "    magnitude_spectrum = np.log(np.abs(f_shift) + 1)\n",
    "    \n",
    "    magnitude_normalized = cv2.normalize(magnitude_spectrum, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    magnitude_bgr = cv2.cvtColor(magnitude_normalized, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    out.write(magnitude_bgr)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Fourier transform video saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb7e2c9",
   "metadata": {},
   "source": [
    "Del PAPPER\n",
    "3. Proposed Algorithm for calculating Image Quality measure\n",
    "3.1 Algorithm for image quality measure\n",
    "Input: Image I of size M×N.\n",
    "Output: Image Quality measure (FM) where FM stands for Frequency Domain Image Blur Measure\n",
    "Step 1: Compute F which is the Fourier Transform representation of image I\n",
    "Step 2: Find Fc which is obtained by shifting the origin of F to centre.\n",
    "Step 3: Calculate AF = abs (Fc) where AF is the absolute value of the centered Fourier transform of image I.\n",
    "Step 4: Calculate M = max (AF) where M is the maximum value of the frequency component in F.\n",
    "Step 5: Calculate TH = the total number of pixels in F whose pixel value > thres, where thres = M/1000.\n",
    "Step 6: Calculate Image Quality measure (FM) from equation (1).\n",
    "FM = TH / (M * N) (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_quality(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    f_transform = np.fft.fft2(gray)\n",
    "    f_shift = np.fft.fftshift(f_transform)\n",
    "    magnitude_spectrum = np.abs(f_shift)\n",
    "    \n",
    "    M = np.max(magnitude_spectrum)\n",
    "    thres = M / 1000\n",
    "    TH = np.sum(magnitude_spectrum > thres)\n",
    "    N = image.shape[0] * image.shape[1]\n",
    "    \n",
    "    FM = TH / (M * N) if M * N != 0 else 0\n",
    "    return FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/focus_video.mov\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "quality_scores = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    quality = calculate_image_quality(frame)\n",
    "    quality_scores.append(quality)\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quality scores over time\n",
    "max = np.argmax(quality_scores) if quality_scores else 0\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(quality_scores, marker='o')\n",
    "plt.axvline(x=max, color='r', linestyle='--', label=f'Max Quality Frame: {max}')\n",
    "plt.title('Image Quality Scores Over Time')\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Quality Score (FM)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f\"Max quality score: {quality_scores[max]:.6f} at frame {max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_quality_roi(image, porcentage_roi=0.05):\n",
    "    im=image.copy()\n",
    "    h, w, _ = im.shape\n",
    "    roi_h, roi_w = int(h * porcentage_roi), int(w * porcentage_roi)\n",
    "    start_h, start_w = (h - roi_h) // 2, (w - roi_w) // 2\n",
    "    roi = im[start_h:start_h + roi_h, start_w:start_w + roi_w]\n",
    "    FM=calculate_image_quality(roi)\n",
    "    \n",
    "    return FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/focus_video.mov\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "quality_scores_5p = []\n",
    "quality_scores_10p = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    quality_5p = calculate_image_quality_roi(frame, porcentage_roi=0.05)\n",
    "    quality_scores_5p.append(quality_5p)\n",
    "    quality_10p = calculate_image_quality_roi(frame, porcentage_roi=0.10)\n",
    "    quality_scores_10p.append(quality_10p)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot quality scores ROI over time and the max value\n",
    "max_5p = np.argmax(quality_scores_5p) if quality_scores_5p else 0\n",
    "max_10p = np.argmax(quality_scores_10p) if quality_scores_10p else 0\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(quality_scores_5p, marker='o', label='5% ROI')\n",
    "plt.plot(quality_scores_10p, marker='x', label='10% ROI')\n",
    "plt.axvline(x=max_5p, color='r', linestyle='--', label=f'Max 5% ROI, frame {max_5p}' if quality_scores_5p else '')\n",
    "plt.axvline(x=max_10p, color='g', linestyle='--', label=f'Max 10% ROI, frame {max_10p}' if quality_scores_10p else '')\n",
    "plt.title('Image Quality Scores Over Time (ROI)')\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Quality Score (FM)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(f\"Max quality score (5% ROI): {quality_scores_5p[max_5p]:.6f} at frame {max_5p}\")\n",
    "print(f\"Max quality score (10% ROI): {quality_scores_10p[max_10p]:.6f} at frame {max_10p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the max quality frame for all the algorithms\n",
    "max = np.argmax(quality_scores) if quality_scores else 0\n",
    "max_5p = np.argmax(quality_scores_5p) if quality_scores_5p else 0\n",
    "max_10p = np.argmax(quality_scores_10p) if quality_scores_10p else 0\n",
    "input_path = \"data/focus_video.mov\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "quality_scores_5p = []\n",
    "quality_scores_10p = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if cap.get(cv2.CAP_PROP_POS_FRAMES) - 1 == max:\n",
    "        max_frame = frame.copy()\n",
    "    if cap.get(cv2.CAP_PROP_POS_FRAMES) - 1 == max_5p:\n",
    "        max_frame_5p = frame.copy()\n",
    "    if cap.get(cv2.CAP_PROP_POS_FRAMES) - 1 == max_10p:\n",
    "        max_frame_10p = frame.copy()\n",
    "cap.release()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(cv2.cvtColor(max_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Max Quality Frame: {max}')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(cv2.cvtColor(max_frame_5p, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Max 5% ROI Frame: {max_5p}')\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cv2.cvtColor(max_frame_10p, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Max 10% ROI Frame: {max_10p}')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_quality_matrix(image, matrix_size=(3, 3), roi_percentage=0.6):\n",
    "    rows, cols = matrix_size\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Definir la ROI centrada\n",
    "    roi_h = int(h * roi_percentage)\n",
    "    roi_w = int(w * roi_percentage)\n",
    "    roi_start_h = (h - roi_h) // 2\n",
    "    roi_start_w = (w - roi_w) // 2\n",
    "    roi_end_h = roi_start_h + roi_h\n",
    "    roi_end_w = roi_start_w + roi_w\n",
    "    \n",
    "    total_rows = rows * 2  # filas totales incluyendo espacios\n",
    "    total_cols = cols * 2  # columnas totales incluyendo espacios\n",
    "    \n",
    "    cell_h = roi_h // total_rows\n",
    "    cell_w = roi_w // total_cols\n",
    "    \n",
    "    quality_matrix = np.zeros((rows, cols))\n",
    "    \n",
    "    total_cells = rows * cols\n",
    "    with tqdm(total=total_cells, desc=\"Processing matrix cells\") as pbar:\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                # Calcular posición saltando una celda\n",
    "                actual_row = i * 2  # Saltar una fila entre celdas\n",
    "                actual_col = j * 2  # Saltar una columna entre celdas\n",
    "                \n",
    "                # Posiciones relativas dentro de la ROI\n",
    "                start_h_roi = actual_row * cell_h\n",
    "                end_h_roi = start_h_roi + cell_h\n",
    "                start_w_roi = actual_col * cell_w\n",
    "                end_w_roi = start_w_roi + cell_w\n",
    "                \n",
    "                # Convertir a coordenadas absolutas de la imagen\n",
    "                start_h = roi_start_h + start_h_roi\n",
    "                end_h = roi_start_h + end_h_roi\n",
    "                start_w = roi_start_w + start_w_roi\n",
    "                end_w = roi_start_w + end_w_roi\n",
    "                \n",
    "                # Asegurar que no excedamos los límites de la ROI\n",
    "                end_h = min(end_h, roi_end_h)\n",
    "                end_w = min(end_w, roi_end_w)\n",
    "                \n",
    "                # Extraer la celda y calcular su calidad\n",
    "                if start_h < roi_end_h and start_w < roi_end_w:\n",
    "                    cell = image[start_h:end_h, start_w:end_w]\n",
    "                    if cell.size > 0: \n",
    "                        quality_matrix[i, j] = calculate_image_quality(cell)\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    return quality_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb294ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/focus_video.mov\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "matrix_sizes = [(3, 3), (5, 7), (7, 5)]\n",
    "roi_percentage = 0.6  # ROI del 60%\n",
    "quality_scores_matrix = {str(size): [] for size in matrix_sizes}\n",
    "\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"Processing {frame_count} frames for matrix quality analysis...\")\n",
    "\n",
    "for frame_idx in tqdm(range(frame_count), desc=\"Calculating matrix quality scores\"):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    for matrix_size in matrix_sizes:\n",
    "        quality_matrix = calculate_image_quality_matrix(frame, matrix_size, roi_percentage)\n",
    "        avg_quality = np.mean(quality_matrix)\n",
    "        quality_scores_matrix[str(matrix_size)].append(avg_quality)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebcbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for i, (matrix_size, scores) in enumerate(quality_scores_matrix.items()):\n",
    "    max_idx = np.argmax(scores) if scores else 0\n",
    "    plt.plot(scores, marker='o', label=f'Matrix {matrix_size}')\n",
    "    plt.axvline(x=max_idx, linestyle='--', alpha=0.7, label=f'Max {matrix_size}: frame {max_idx}')\n",
    "\n",
    "plt.title('Image Quality Scores Over Time (Matrix Focus)')\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Average Quality Score (FM)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02426a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matrix_overlay(image, matrix_size, quality_matrix, roi_percentage=0.6):\n",
    "    \"\"\"\n",
    "    Visualiza la matriz de calidad superpuesta en la imagen con patrón equiespaciado\n",
    "    dentro de una ROI (una celda sí, una celda no)\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    rows, cols = matrix_size\n",
    "    \n",
    "    # Calcular ROI centrada\n",
    "    roi_h = int(h * roi_percentage)\n",
    "    roi_w = int(w * roi_percentage)\n",
    "    roi_start_h = (h - roi_h) // 2\n",
    "    roi_start_w = (w - roi_w) // 2\n",
    "    roi_end_h = roi_start_h + roi_h\n",
    "    roi_end_w = roi_start_w + roi_w\n",
    "    \n",
    "    total_rows = rows * 2\n",
    "    total_cols = cols * 2\n",
    "    \n",
    "    cell_h = roi_h // total_rows\n",
    "    cell_w = roi_w // total_cols\n",
    "    \n",
    "    overlay = image.copy()\n",
    "    \n",
    "    total_cells = rows * cols\n",
    "    with tqdm(total=total_cells, desc=\"Drawing matrix overlay\") as pbar:\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                # Calcular posición con el patrón equiespaciado dentro de la ROI\n",
    "                actual_row = i * 2\n",
    "                actual_col = j * 2\n",
    "                \n",
    "                # Posiciones relativas dentro de la ROI\n",
    "                start_h_roi = actual_row * cell_h\n",
    "                end_h_roi = start_h_roi + cell_h\n",
    "                start_w_roi = actual_col * cell_w\n",
    "                end_w_roi = start_w_roi + cell_w\n",
    "                \n",
    "                # Convertir a coordenadas absolutas\n",
    "                start_h = roi_start_h + start_h_roi\n",
    "                end_h = roi_start_h + end_h_roi\n",
    "                start_w = roi_start_w + start_w_roi\n",
    "                end_w = roi_start_w + end_w_roi\n",
    "                \n",
    "                # Asegurar que no excedamos los límites de la ROI\n",
    "                end_h = min(end_h, roi_end_h)\n",
    "                end_w = min(end_w, roi_end_w)\n",
    "                \n",
    "                quality_val = quality_matrix[i, j]\n",
    "                max_quality = np.max(quality_matrix)\n",
    "                \n",
    "                # Color basado en la calidad relativa\n",
    "                intensity = int(255 * (quality_val / max_quality)) if max_quality > 0 else 0\n",
    "                color = (0, intensity, 255 - intensity)  # De azul (baja calidad) a verde (alta calidad)\n",
    "                \n",
    "                # Dibujar rectángulo y texto\n",
    "                cv2.rectangle(overlay, (start_w, start_h), (end_w, end_h), color, 2)\n",
    "                cv2.putText(overlay, f'{quality_val:.3f}', (start_w + 5, start_h + 20), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "best_score = 0\n",
    "best_matrix_size = None\n",
    "for matrix_key, scores in quality_scores_matrix.items():\n",
    "    if scores and np.max(scores) > best_score:\n",
    "        best_score = np.max(scores)\n",
    "        best_matrix_size = matrix_key\n",
    "\n",
    "best_frame_idx = np.argmax(quality_scores_matrix[best_matrix_size]) if best_matrix_size else 0\n",
    "\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, best_frame_idx)\n",
    "ret, best_frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if ret:\n",
    "    matrix_size = eval(best_matrix_size)\n",
    "    quality_matrix = calculate_image_quality_matrix(best_frame, matrix_size, roi_percentage=0.6)\n",
    "    overlay_image = visualize_matrix_overlay(best_frame, matrix_size, quality_matrix, roi_percentage=0.6)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(best_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Best Quality Frame: {best_frame_idx}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(cv2.cvtColor(overlay_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'Matrix {matrix_size} en ROI 60%')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(quality_matrix, cmap='hot', interpolation='nearest')\n",
    "    plt.title('Quality Matrix Heatmap')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e620ba3",
   "metadata": {},
   "source": [
    "## 2. Cambiar la métrica de enfoque\n",
    "Eligiendo uno de los algoritmos explicados en el apéndice de: **\"Analysis of focus measure operators in shape from focus\"**.\n",
    "\n",
    "El algoritmo de detección a implementar debe detectar y devolver los puntos de máximo enfoque de manera automática.\n",
    "\n",
    "### Resultados esperados:\n",
    "- Matriz de enfoque superpuesta a uno de los frames del video\n",
    "- Gráfico de la métrica asociado al experimento\n",
    "\n",
    "### Puntos extra:\n",
    "Aplicar unsharp masking para expandir la zona de enfoque y recalcular la métrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b75bd",
   "metadata": {},
   "source": [
    "Analizando las conclusiones del paper **\"Analysis of focus measure operators in shape from focus\"** vemos que el mejor operador para medir el enfoque cuando hay ruido gauseano es el gray-level variance (STA3, en el paper) que sufre poco al aumentar el ruido.\n",
    "En cambio el mejor operador en generel es el Laplacian modificado (LAP2 en el paper). Este operador es sensible al ruido por que realiza una segunda derivada, pero es el que mejor resultados da en condiciones ideales.\n",
    "Por lo explicado anteriormente se van a implementar ambos y comparar resultados para nuestro video.\n",
    "\n",
    "## 1. Operador (Varianza - STA3)\n",
    "\n",
    "STA3(i, j) = (1 / N) * SUM((x, y) en W_i,j) [ (I(x, y) - mu_i,j) ^ 2 ]\n",
    "mu_i,j = (1 / N) * SUM((x, y) en W_i,j) [ I(x, y) ]\n",
    "\n",
    "\n",
    "## 2. Operador (Laplaciano Modificado - LAP2)\n",
    "LAP2(x, y) = SUM((i, j) en W_x,y) [ DmI(i, j) ]\n",
    "DmI(i, j) = | I * LX | + | I * LY |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sta3_focus_measure(image: np.ndarray, window_size: int = 3) -> np.ndarray:\n",
    "    \"\"\"Calculate STA3 (Gray-level Variance) focus measure.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (color or grayscale)\n",
    "        window_size: Size of analysis window\n",
    "        \n",
    "    Returns:\n",
    "        Focus map array\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image.copy()\n",
    "    gray = gray.astype(np.float32)\n",
    "    \n",
    "    h, w = gray.shape\n",
    "    half_window = window_size // 2\n",
    "    focus_map = np.zeros((h, w), dtype=np.float32)\n",
    "    \n",
    "    total_pixels = (h - 2 * half_window) * (w - 2 * half_window)\n",
    "    \n",
    "    with tqdm(total=total_pixels, desc=\"Calculating STA3 focus\") as pbar:\n",
    "        for i in range(half_window, h - half_window):\n",
    "            for j in range(half_window, w - half_window):\n",
    "                # Extract window\n",
    "                window = gray[i-half_window:i+half_window+1, j-half_window:j+half_window+1]\n",
    "                \n",
    "                # Calculate variance\n",
    "                window_mean = np.mean(window)\n",
    "                variance = np.mean((window - window_mean) ** 2)\n",
    "                focus_map[i, j] = variance\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    return focus_map\n",
    "\n",
    "def calculate_lap2_focus_measure(image: np.ndarray, window_size: int = 3) -> np.ndarray:\n",
    "    \"\"\"Calculate LAP2 (Modified Laplacian) focus measure.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (color or grayscale)\n",
    "        window_size: Size of analysis window\n",
    "        \n",
    "    Returns:\n",
    "        Focus map array\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image.copy()\n",
    "    gray = gray.astype(np.float32)\n",
    "    \n",
    "    h, w = gray.shape\n",
    "    half_window = window_size // 2\n",
    "    \n",
    "    # Laplacian masks\n",
    "    mask_x = np.array([-1, 2, -1], dtype=np.float32)\n",
    "    mask_y = np.array([[-1], [2], [-1]], dtype=np.float32)\n",
    "    \n",
    "    # Calculate derivatives\n",
    "    derivative_image = np.zeros((h, w), dtype=np.float32)\n",
    "    \n",
    "    total_pixels_dmi = (h - 2) * (w - 2)\n",
    "    with tqdm(total=total_pixels_dmi, desc=\"Calculating LAP2 derivatives\") as pbar:\n",
    "        for i in range(1, h - 1):\n",
    "            for j in range(1, w - 1):\n",
    "                # Horizontal and vertical derivatives\n",
    "                dx = np.sum(gray[i, j-1:j+2] * mask_x)\n",
    "                dy = np.sum(gray[i-1:i+2, j] * mask_y.flatten())\n",
    "                derivative_image[i, j] = abs(dx) + abs(dy)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Accumulate in windows\n",
    "    focus_map = np.zeros((h, w), dtype=np.float32)\n",
    "    total_windows = (h - 2 * half_window) * (w - 2 * half_window)\n",
    "    \n",
    "    with tqdm(total=total_windows, desc=\"Calculating LAP2 windows\") as pbar:\n",
    "        for x in range(half_window, h - half_window):\n",
    "            for y in range(half_window, w - half_window):\n",
    "                window = derivative_image[x-half_window:x+half_window+1, y-half_window:y+half_window+1]\n",
    "                focus_map[x, y] = np.sum(window)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return focus_map\n",
    "\n",
    "def max_focus_points(focus_map: np.ndarray) -> Tuple[List[Tuple[int, int]], float]:\n",
    "    \"\"\"Get pixels with maximum focus value.\n",
    "    \n",
    "    Args:\n",
    "        focus_map: Focus measurement map\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (list of max focus points, max focus value)\n",
    "    \"\"\"\n",
    "    max_value = np.max(focus_map)\n",
    "    max_positions = np.where(focus_map == max_value)\n",
    "    max_points = list(zip(max_positions[0], max_positions[1]))\n",
    "    \n",
    "    return max_points, max_value\n",
    "\n",
    "def create_focus_points_video(input_path, output_path, algorithm='sta3', window_size=3):\n",
    "    \"\"\"\n",
    "    Genera un video que muestra los puntos de máximo enfoque superpuestos al video original\n",
    "    \n",
    "    Parameters:\n",
    "    - input_path: ruta del video de entrada\n",
    "    - output_path: ruta del video de salida\n",
    "    - algorithm: 'sta3' para Gray-level Variance o 'lap2' para Laplaciano Modificado\n",
    "    - window_size: tamaño de la ventana para el algoritmo\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Seleccionar el algoritmo\n",
    "    if algorithm.lower() == 'sta3':\n",
    "        focus_function = calculate_sta3_focus_measure\n",
    "        algorithm_name = \"STA3 (Gray-level Variance)\"\n",
    "    elif algorithm.lower() == 'lap2':\n",
    "        focus_function = calculate_lap2_focus_measure\n",
    "        algorithm_name = \"LAP2 (Laplaciano Modificado)\"\n",
    "    else:\n",
    "        raise ValueError(\"Algorithm must be 'sta3' or 'lap2'\")\n",
    "    \n",
    "    \n",
    "    for frame_idx in tqdm(range(frame_count), desc=f\"Processing {algorithm_name}\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        focus_map = focus_function(frame, window_size)\n",
    "        max_points, max_value = max_focus_points(focus_map)\n",
    "        \n",
    "        output_frame = frame.copy()\n",
    "        \n",
    "        # Dibujar los puntos de máximo enfoque\n",
    "        for point in max_points:\n",
    "            row, col = point\n",
    "            cv2.circle(output_frame, (col, row), 5, (0, 0, 255), -1)\n",
    "            cv2.circle(output_frame, (col, row), 15, (0, 0, 255), 2)\n",
    "        \n",
    "        # Texto informativo\n",
    "        text = f'Frame: {frame_idx} | {algorithm_name} | Max Focus: {max_value:.2f} | Points: {len(max_points)}'\n",
    "        cv2.putText(output_frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        cv2.putText(output_frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "\n",
    "        out.write(output_frame)\n",
    "\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Video con puntos de enfoque guardado en: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1af9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar videos con ambos algoritmos para comparar\n",
    "input_video = \"data/focus_video.mov\"\n",
    "\n",
    "# Video con algoritmo STA3 (Gray-level Variance)\n",
    "output_video_sta3 = \"outputs/focus_points_video_STA3.mp4\"\n",
    "create_focus_points_video(input_video, output_video_sta3, algorithm='sta3', window_size=3)\n",
    "\n",
    "# Video con algoritmo LAP2 (Laplaciano Modificado)\n",
    "output_video_lap2 = \"outputs/focus_points_video_LAP2.mp4\"\n",
    "create_focus_points_video(input_video, output_video_lap2, algorithm='lap2', window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación visual de ambos algoritmos en un frame específico\n",
    "input_path = \"data/focus_video.mov\"\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "# Usar el frame con mejor calidad que encontramos anteriormente\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, best_frame_idx)\n",
    "ret, test_frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if ret:\n",
    "    # Calcular mapas de enfoque con ambos algoritmos\n",
    "    focus_map_sta3 = calculate_sta3_focus_measure(test_frame, window_size=3)\n",
    "    focus_map_lap2 = calculate_lap2_focus_measure(test_frame, window_size=3)\n",
    "    \n",
    "    # Encontrar puntos de máximo enfoque\n",
    "    max_points_sta3, max_value_sta3 = max_focus_points(focus_map_sta3)\n",
    "    max_points_lap2, max_value_lap2 = max_focus_points(focus_map_lap2)\n",
    "    \n",
    "    # Crear imágenes con puntos superpuestos\n",
    "    frame_sta3 = test_frame.copy()\n",
    "    frame_lap2 = test_frame.copy()\n",
    "    \n",
    "    # Dibujar puntos STA3\n",
    "    for point in max_points_sta3:\n",
    "        row, col = point\n",
    "        cv2.circle(frame_sta3, (col, row), 5, (0, 0, 255), -1)\n",
    "        cv2.circle(frame_sta3, (col, row), 15, (0, 0, 255), 2)\n",
    "    \n",
    "    # Dibujar puntos LAP2\n",
    "    for point in max_points_lap2:\n",
    "        row, col = point\n",
    "        cv2.circle(frame_lap2, (col, row), 5, (0, 255, 0), -1)\n",
    "        cv2.circle(frame_lap2, (col, row), 15, (0, 255, 0), 2)\n",
    "    \n",
    "    # Visualización comparativa\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # Fila superior: Imágenes originales con puntos\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(frame_sta3, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'STA3 - Max Focus Points\\nMax Value: {max_value_sta3:.2f}, Points: {len(max_points_sta3)}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(cv2.cvtColor(frame_lap2, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f'LAP2 - Max Focus Points\\nMax Value: {max_value_lap2:.2f}, Points: {len(max_points_lap2)}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.imshow(cv2.cvtColor(test_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Imagen Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Fila inferior: Mapas de enfoque\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(focus_map_sta3, cmap='hot', interpolation='nearest')\n",
    "    plt.title('Mapa de Enfoque STA3')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(focus_map_lap2, cmap='hot', interpolation='nearest')\n",
    "    plt.title('Mapa de Enfoque LAP2')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    # Comparación estadística\n",
    "    plt.text(0.1, 0.8, f'Estadísticas de Comparación:', fontsize=14, fontweight='bold')\n",
    "    plt.text(0.1, 0.7, f'STA3:', fontsize=12, fontweight='bold')\n",
    "    plt.text(0.1, 0.65, f'  - Max Value: {max_value_sta3:.4f}', fontsize=10)\n",
    "    plt.text(0.1, 0.6, f'  - Puntos Max: {len(max_points_sta3)}', fontsize=10)\n",
    "    plt.text(0.1, 0.55, f'  - Media: {np.mean(focus_map_sta3):.4f}', fontsize=10)\n",
    "    plt.text(0.1, 0.5, f'  - Std: {np.std(focus_map_sta3):.4f}', fontsize=10)\n",
    "    \n",
    "    plt.text(0.1, 0.4, f'LAP2:', fontsize=12, fontweight='bold')\n",
    "    plt.text(0.1, 0.35, f'  - Max Value: {max_value_lap2:.4f}', fontsize=10)\n",
    "    plt.text(0.1, 0.3, f'  - Puntos Max: {len(max_points_lap2)}', fontsize=10)\n",
    "    plt.text(0.1, 0.25, f'  - Media: {np.mean(focus_map_lap2):.4f}', fontsize=10)\n",
    "    plt.text(0.1, 0.2, f'  - Std: {np.std(focus_map_lap2):.4f}', fontsize=10)\n",
    "    \n",
    "    plt.text(0.1, 0.1, f'Frame analizado: {best_frame_idx}', fontsize=10)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Comparación de Algoritmos de Detección de Enfoque: STA3 vs LAP2', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementación de Unsharp Masking para expandir zonas de enfoque\n",
    "\n",
    "def apply_unsharp_mask(\n",
    "    image: np.ndarray, \n",
    "    kernel_size: Tuple[int, int] = (5, 5), \n",
    "    sigma: float = 1.0, \n",
    "    amount: float = 1.5, \n",
    "    threshold: float = 0\n",
    ") -> np.ndarray:\n",
    "\n",
    "    def apply_single_channel(channel: np.ndarray) -> np.ndarray:\n",
    "        channel_float = channel.astype(np.float32)\n",
    "        \n",
    "        # Create Gaussian blur\n",
    "        gaussian = cv2.GaussianBlur(channel_float, kernel_size, sigma)\n",
    "        \n",
    "        # Create unsharp mask\n",
    "        unsharp_mask = channel_float - gaussian\n",
    "        \n",
    "        # Apply threshold if specified\n",
    "        if threshold > 0:\n",
    "            unsharp_mask = np.where(np.abs(unsharp_mask) < threshold, 0, unsharp_mask)\n",
    "        \n",
    "        # Apply mask with amount factor\n",
    "        sharpened = channel_float + (amount * unsharp_mask)\n",
    "        \n",
    "        return np.clip(sharpened, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        # Apply to each channel\n",
    "        result = np.zeros_like(image)\n",
    "        for i in range(image.shape[2]):\n",
    "            result[:, :, i] = apply_single_channel(image[:, :, i])\n",
    "        return result\n",
    "    else:\n",
    "        return apply_single_channel(image)\n",
    "\n",
    "def calculate_focus_with_unsharp(\n",
    "    image: np.ndarray, \n",
    "    algorithm: str = 'sta3', \n",
    "    window_size: int = 3, \n",
    "    unsharp_params: Optional[Dict] = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "    if unsharp_params is None:\n",
    "        unsharp_params = {\n",
    "            'kernel_size': (5, 5),\n",
    "            'sigma': 1.0,\n",
    "            'amount': 1.5,\n",
    "            'threshold': 0\n",
    "        }\n",
    "    \n",
    "    # Apply unsharp masking\n",
    "    enhanced_image = apply_unsharp_mask(image, **unsharp_params)\n",
    "    \n",
    "    # Calculate focus on enhanced image\n",
    "    if algorithm.lower() == 'sta3':\n",
    "        focus_map = calculate_sta3_focus_measure(enhanced_image, window_size)\n",
    "    elif algorithm.lower() == 'lap2':\n",
    "        focus_map = calculate_lap2_focus_measure(enhanced_image, window_size)\n",
    "    else:\n",
    "        raise ValueError(\"Algorithm must be 'sta3' or 'lap2'\")\n",
    "    \n",
    "    return focus_map, enhanced_image\n",
    "\n",
    "# Demostración con diferentes parámetros de Unsharp Masking\n",
    "def demo_unsharp_parameters(image, frame_idx):\n",
    "    \"\"\"\n",
    "    Demuestra el efecto de diferentes parámetros de Unsharp Masking\n",
    "    \"\"\"\n",
    "    # Diferentes configuraciones de Unsharp Masking\n",
    "    unsharp_configs = [\n",
    "        {'name': 'Sin Unsharp', 'params': None, 'apply_unsharp': False},\n",
    "        {'name': 'Suave', 'params': {'kernel_size': (3, 3), 'sigma': 0.5, 'amount': 1.2, 'threshold': 0}, 'apply_unsharp': True},\n",
    "        {'name': 'Moderado', 'params': {'kernel_size': (5, 5), 'sigma': 1.0, 'amount': 1.5, 'threshold': 0}, 'apply_unsharp': True},\n",
    "        {'name': 'Intenso', 'params': {'kernel_size': (7, 7), 'sigma': 1.5, 'amount': 2.0, 'threshold': 0}, 'apply_unsharp': True}\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, config in enumerate(unsharp_configs):\n",
    "        # Calcular enfoque con ambos algoritmos\n",
    "        if config['apply_unsharp']:\n",
    "            focus_map_sta3, enhanced_image = calculate_focus_with_unsharp(\n",
    "                image, 'sta3', window_size=3, unsharp_params=config['params']\n",
    "            )\n",
    "            focus_map_lap2, _ = calculate_focus_with_unsharp(\n",
    "                image, 'lap2', window_size=3, unsharp_params=config['params']\n",
    "            )\n",
    "        else:\n",
    "            # Sin unsharp masking\n",
    "            focus_map_sta3 = calculate_sta3_focus_measure(image, window_size=3)\n",
    "            focus_map_lap2 = calculate_lap2_focus_measure(image, window_size=3)\n",
    "            enhanced_image = image\n",
    "        \n",
    "        # Obtener puntos de máximo enfoque\n",
    "        max_points_sta3, max_value_sta3 = max_focus_points(focus_map_sta3)\n",
    "        max_points_lap2, max_value_lap2 = max_focus_points(focus_map_lap2)\n",
    "        \n",
    "        # Guardar resultados para estadísticas\n",
    "        results.append({\n",
    "            'name': config['name'],\n",
    "            'sta3_max': max_value_sta3,\n",
    "            'sta3_mean': np.mean(focus_map_sta3),\n",
    "            'sta3_points': len(max_points_sta3),\n",
    "            'lap2_max': max_value_lap2,\n",
    "            'lap2_mean': np.mean(focus_map_lap2),\n",
    "            'lap2_points': len(max_points_lap2)\n",
    "        })\n",
    "        \n",
    "        # Visualización\n",
    "        # Imagen realzada\n",
    "        plt.subplot(4, 4, idx * 4 + 1)\n",
    "        if len(enhanced_image.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(enhanced_image, cmap='gray')\n",
    "        plt.title(f'{config[\"name\"]}\\nImagen Realzada')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Mapa de enfoque STA3\n",
    "        plt.subplot(4, 4, idx * 4 + 2)\n",
    "        plt.imshow(focus_map_sta3, cmap='hot', interpolation='nearest')\n",
    "        plt.title(f'STA3 Focus Map\\nMax: {max_value_sta3:.2f}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Mapa de enfoque LAP2\n",
    "        plt.subplot(4, 4, idx * 4 + 3)\n",
    "        plt.imshow(focus_map_lap2, cmap='hot', interpolation='nearest')\n",
    "        plt.title(f'LAP2 Focus Map\\nMax: {max_value_lap2:.2f}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Puntos de máximo enfoque superpuestos\n",
    "        plt.subplot(4, 4, idx * 4 + 4)\n",
    "        overlay = enhanced_image.copy()\n",
    "        \n",
    "        # Dibujar puntos STA3 en azul\n",
    "        for point in max_points_sta3:\n",
    "            row, col = point\n",
    "            cv2.circle(overlay, (col, row), 3, (255, 0, 0), -1)\n",
    "        \n",
    "        # Dibujar puntos LAP2 en rojo\n",
    "        for point in max_points_lap2:\n",
    "            row, col = point\n",
    "            cv2.circle(overlay, (col, row), 3, (0, 0, 255), -1)\n",
    "        \n",
    "        if len(overlay.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(overlay, cmap='gray')\n",
    "        plt.title(f'Max Focus Points\\nSTA3: {len(max_points_sta3)}, LAP2: {len(max_points_lap2)}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Efecto del Unsharp Masking en Detección de Enfoque - Frame {frame_idx}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4db5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar demostración de Unsharp Masking en el frame de mejor calidad\n",
    "print(f\"Aplicando Unsharp Masking al frame {best_frame_idx} (mejor calidad detectada)\")\n",
    "unsharp_results = demo_unsharp_parameters(best_frame, best_frame_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar video con Unsharp Masking aplicado para mejor visualización\n",
    "def create_focus_points_video_with_unsharp(input_path, output_path, algorithm='sta3', \n",
    "                                          window_size=3, unsharp_params=None, max_frames=None):\n",
    "    if unsharp_params is None:\n",
    "        unsharp_params = {'kernel_size': (5, 5), 'sigma': 1.0, 'amount': 1.5, 'threshold': 0}\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if max_frames:\n",
    "        frame_count = min(max_frames, frame_count)\n",
    "    \n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    algorithm_name = \"STA3 + Unsharp\" if algorithm.lower() == 'sta3' else \"LAP2 + Unsharp\"\n",
    "    \n",
    "    print(f\"Generando video con {algorithm_name} en {frame_count} frames...\")\n",
    "    \n",
    "    for frame_idx in tqdm(range(frame_count), desc=f\"Generating {algorithm_name} video\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Aplicar unsharp masking y calcular enfoque\n",
    "        focus_map, enhanced_image = calculate_focus_with_unsharp(\n",
    "            frame, algorithm, window_size, unsharp_params\n",
    "        )\n",
    "        max_points, max_value = max_focus_points(focus_map)\n",
    "        \n",
    "        # Usar la imagen realzada para el video\n",
    "        output_frame = enhanced_image.copy()\n",
    "        \n",
    "        # Dibujar los puntos de máximo enfoque\n",
    "        for point in max_points:\n",
    "            row, col = point\n",
    "            cv2.circle(output_frame, (col, row), 5, (0, 255, 255), -1)  # Amarillo para distinguir\n",
    "            cv2.circle(output_frame, (col, row), 15, (0, 255, 255), 2)\n",
    "        \n",
    "        # Texto informativo\n",
    "        text1 = f'Frame: {frame_idx} | {algorithm_name}'\n",
    "        text2 = f'Max Focus: {max_value:.2f} | Points: {len(max_points)}'\n",
    "        text3 = f'Unsharp: σ={unsharp_params[\"sigma\"]}, Amount={unsharp_params[\"amount\"]}'\n",
    "        \n",
    "        cv2.putText(output_frame, text1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(output_frame, text1, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)\n",
    "        cv2.putText(output_frame, text2, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(output_frame, text2, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)\n",
    "        cv2.putText(output_frame, text3, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "        cv2.putText(output_frame, text3, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "        out.write(output_frame)\n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Video con Unsharp Masking guardado en: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Generar videos de demostración con Unsharp Masking\n",
    "input_video = \"data/focus_video.mov\"\n",
    "\n",
    "# Configuraciones de Unsharp Masking para demostrar\n",
    "unsharp_moderate = {'kernel_size': (5, 5), 'sigma': 1.0, 'amount': 1.5, 'threshold': 0}\n",
    "unsharp_intense = {'kernel_size': (7, 7), 'sigma': 1.5, 'amount': 2.0, 'threshold': 0}\n",
    "\n",
    "print(\"Generando videos con Unsharp Masking...\")\n",
    "\n",
    "# Video STA3 + Unsharp Masking moderado\n",
    "output_sta3_unsharp = \"outputs/focus_points_STA3_Unsharp_Moderate.mp4\"\n",
    "create_focus_points_video_with_unsharp(\n",
    "    input_video, output_sta3_unsharp, 'sta3', 3, unsharp_moderate, max_frames=60\n",
    ")\n",
    "\n",
    "# Video LAP2 + Unsharp Masking intenso  \n",
    "output_lap2_unsharp = \"outputs/focus_points_LAP2_Unsharp_Intense.mp4\"\n",
    "create_focus_points_video_with_unsharp(\n",
    "    input_video, output_lap2_unsharp, 'lap2', 3, unsharp_intense, max_frames=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a76bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_unsharp_masking_steps(\n",
    "    image: np.ndarray, \n",
    "    kernel_size: Tuple[int, int] = (5, 5), \n",
    "    sigma: float = 1.0, \n",
    "    amount: float = 1.5, \n",
    "    threshold: float = 0, \n",
    "    title_prefix: str = \"\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Visualizes Unsharp Masking process and returns enhanced image.\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image.copy()\n",
    "    gray_float = gray.astype(np.float32)\n",
    "    \n",
    "    gaussian = cv2.GaussianBlur(gray_float, kernel_size, sigma)\n",
    "    unsharp_mask = gray_float - gaussian\n",
    "    \n",
    "    if threshold > 0:\n",
    "        unsharp_mask = np.where(np.abs(unsharp_mask) < threshold, 0, unsharp_mask)\n",
    "    \n",
    "    enhanced_mask = amount * unsharp_mask\n",
    "    final_result = np.clip(gray_float + enhanced_mask, 0, 255)\n",
    "    \n",
    "    # Compact visualization\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    fig.suptitle(f'{title_prefix}Unsharp Masking (σ={sigma}, amount={amount})', fontsize=14)\n",
    "    \n",
    "    axes[0].imshow(gray_float, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(gaussian, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[1].set_title('Gaussian Blur')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(unsharp_mask + 128, cmap='RdBu_r', vmin=0, vmax=255)\n",
    "    axes[2].set_title('Unsharp Mask')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    axes[3].imshow(final_result, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[3].set_title('Enhanced')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    contrast_improvement = np.std(final_result) / np.std(gray_float) if np.std(gray_float) > 0 else 1\n",
    "    print(f\"{title_prefix}Contrast improvement: {contrast_improvement:.3f}x\")\n",
    "    \n",
    "    return final_result.astype(np.uint8)\n",
    "\n",
    "def analyze_unsharp_on_different_focus_frames() -> Tuple[Optional[np.ndarray], Optional[np.ndarray], float, float]:\n",
    "    \"\"\"Analyze Unsharp Masking effects on frames with different focus quality.\"\"\"\n",
    "    cap = cv2.VideoCapture(\"data/focus_video.mov\")\n",
    "    \n",
    "    best_focus_idx = np.argmax(quality_scores)\n",
    "    poor_focus_idx = len(quality_scores) // 4\n",
    "    \n",
    "    frames = {}\n",
    "    for idx, name in [(best_focus_idx, 'good'), (poor_focus_idx, 'poor')]:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames[name] = frame\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if len(frames) != 2:\n",
    "        return None, None, 0, 0\n",
    "    \n",
    "    print(f\"Analyzing frames: {best_focus_idx} (good) vs {poor_focus_idx} (poor)\")\n",
    "    \n",
    "    unsharp_params = {'kernel_size': (5, 5), 'sigma': 1.0, 'amount': 1.5, 'threshold': 0}\n",
    "    \n",
    "    enhanced_frames = {}\n",
    "    improvements = {}\n",
    "    \n",
    "    for frame_type, (idx, frame) in [('good', (best_focus_idx, frames['good'])), \n",
    "                                     ('poor', (poor_focus_idx, frames['poor']))]:\n",
    "        \n",
    "        print(f\"\\nFrame {idx} - {frame_type.upper()} FOCUS:\")\n",
    "        enhanced = visualize_unsharp_masking_steps(\n",
    "            frame, title_prefix=f\"Frame {idx} ({frame_type}) - \", **unsharp_params\n",
    "        )\n",
    "        \n",
    "        focus_before = calculate_sta3_focus_measure(frame, window_size=3)\n",
    "        focus_after = calculate_sta3_focus_measure(enhanced, window_size=3)\n",
    "        \n",
    "        _, max_before = max_focus_points(focus_before)\n",
    "        _, max_after = max_focus_points(focus_after)\n",
    "        \n",
    "        improvement = max_after / max_before if max_before > 0 else 1\n",
    "        \n",
    "        enhanced_frames[frame_type] = enhanced\n",
    "        improvements[frame_type] = improvement\n",
    "        \n",
    "        print(f\"Focus: {max_before:.2f} → {max_after:.2f} ({improvement:.3f}x)\")\n",
    "    \n",
    "    relative_effectiveness = improvements['poor'] / improvements['good'] if improvements['good'] > 0 else 1\n",
    "    print(f\"\\nUnsharp Masking relative effectiveness (poor/good): {relative_effectiveness:.3f}x\")\n",
    "    \n",
    "    return enhanced_frames['good'], enhanced_frames['poor'], improvements['good'], improvements['poor']\n",
    "\n",
    "enhanced_good, enhanced_poor, improvement_good, improvement_poor = analyze_unsharp_on_different_focus_frames()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
